{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dual_Directional_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1cNrNSOP2qPyuu8lrHp91MQIn2ccaSFOD",
      "authorship_tag": "ABX9TyNwrIEAQhhmJ3cNiG8EACJO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VedantDere0104/Dual_Directional_GAN/blob/main/Dual_Directional_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8ARAOPwRBCg"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrGl__JZRFZM"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchsummary import summary"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_UCWOgcSj5c"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "in_channels = 3\n",
        "out_channels = 3\n",
        "out_channels_disc = 1\n",
        "z_dim = 512"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whFEg1tCUQnp"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=2, size=(1, 28, 28)):\n",
        "  image_shifted = image_tensor\n",
        "  image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
        "  image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "  plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "  plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGM5g7gMUSjc"
      },
      "source": [
        "def crop(image, new_shape):\n",
        "    middle_height = image.shape[2] // 2\n",
        "    middle_width = image.shape[3] // 2\n",
        "    starting_height = middle_height - new_shape[2] // 2\n",
        "    final_height = starting_height + new_shape[2]\n",
        "    starting_width = middle_width - new_shape[3] // 2\n",
        "    final_width = starting_width + new_shape[3]\n",
        "    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\n",
        "    return cropped_image"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eqeb_F8RQoN"
      },
      "source": [
        "class Conv(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels , \n",
        "               kernel_size = 3 , \n",
        "               stride = 1 , \n",
        "               padding = 1 ,\n",
        "               use_norm = True ,\n",
        "               use_activation = True , \n",
        "               use_dropout = False , \n",
        "               use_pool = True):\n",
        "    super(Conv , self).__init__()\n",
        "\n",
        "    self.use_norm = use_norm\n",
        "    self.use_activation = use_activation\n",
        "    self.use_dropout = use_dropout\n",
        "    self.use_pool = use_pool\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels , out_channels , kernel_size , stride , padding)\n",
        "\n",
        "    if self.use_norm:\n",
        "      self.norm = nn.BatchNorm2d(out_channels)\n",
        "    \n",
        "    if self.use_activation:\n",
        "      self.activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    if self.use_dropout:\n",
        "      self.dropout = nn.Dropout()\n",
        "\n",
        "    if self.use_pool:\n",
        "      self.maxpool = nn.MaxPool2d(kernel_size=(2 , 2) , stride=(2 , 2))\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = self.conv1(x)\n",
        "    if self.use_norm:\n",
        "      x = self.norm(x)\n",
        "    if self.use_activation:\n",
        "      x = self.activation(x)\n",
        "    if self.use_dropout:\n",
        "      x = self.dropout(x)\n",
        "    if self.use_pool:\n",
        "      x = self.maxpool(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erjk4MvqSUzh"
      },
      "source": [
        "conv = Conv(3 , 32).to(device)\n",
        "summary(conv , (3 , 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI3D31BXSvrt"
      },
      "source": [
        "class ConvT(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels , \n",
        "               kernel_size = (2 , 2) , \n",
        "               stride = (2 , 2) , \n",
        "               padding = 0 , \n",
        "               use_norm = True , \n",
        "               use_activation = True , \n",
        "               use_dropout = False):\n",
        "    super(ConvT , self).__init__()\n",
        "\n",
        "    self.use_norm = use_norm\n",
        "    self.use_activation = use_activation\n",
        "    self.use_dropout = use_dropout\n",
        "\n",
        "    self.convT1 = nn.ConvTranspose2d(in_channels * 2 , out_channels , kernel_size, stride , padding)\n",
        "\n",
        "    if self.use_norm:\n",
        "      self.norm = nn.InstanceNorm2d(out_channels)\n",
        "    if self.use_activation:\n",
        "      self.activation = nn.LeakyReLU(0.2)\n",
        "    if self.use_dropout:\n",
        "      self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self , x , y):\n",
        "    y = crop(y , x.shape)\n",
        "    x = torch.cat([x , y] , dim=1)\n",
        "    x = self.convT1(x)\n",
        "\n",
        "    if self.use_norm:\n",
        "      x = self.norm(x)\n",
        "    if self.use_activation:\n",
        "      x = self.activation(x)\n",
        "    if self.use_dropout:\n",
        "      x = self.dropout(x)\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmu-tzFcUYlY"
      },
      "source": [
        "convT = ConvT(128 , 32).to(device)\n",
        "x = torch.randn(2 , 128 , 256 , 256).to(device)\n",
        "y = torch.randn(2 , 128 , 512 , 512).to(device)\n",
        "z = convT(x , y)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb6gwij7Ud1l"
      },
      "source": [
        "class Encoder_Source(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               z_dim = 512 ,\n",
        "               hidden_dim = 32):\n",
        "    super(Encoder_Source , self).__init__()\n",
        "\n",
        "    self.conv1 = Conv(in_channels , hidden_dim , use_norm=False)\n",
        "    self.conv2 = Conv(hidden_dim , hidden_dim * 2 , use_norm=False)\n",
        "    self.conv3 = Conv(hidden_dim * 2 , hidden_dim * 4)\n",
        "    self.conv4 = Conv(hidden_dim * 4 , hidden_dim * 8)\n",
        "    self.conv5 = Conv(hidden_dim * 8 , hidden_dim * 16)\n",
        "    self.conv6 = Conv(hidden_dim * 16 , hidden_dim * 32)\n",
        "    self.conv7 = Conv(hidden_dim * 32 , hidden_dim * 64 , use_norm=False)\n",
        "    self.conv8 = Conv(hidden_dim * 64,  z_dim , use_norm=False)\n",
        "\n",
        "  def forward(self ,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.conv6(x)\n",
        "    x = self.conv7(x)\n",
        "    x = self.conv8(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Wxus_GjJ0N"
      },
      "source": [
        "encoder_source = Encoder_Source(3).to(device)\n",
        "summary(encoder_source , (3 , 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPx8Nko1jK3K"
      },
      "source": [
        "class Encoder_Target(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels , \n",
        "               hidden_dim = 32):\n",
        "    super(Encoder_Target , self).__init__()\n",
        "\n",
        "    self.conv1 = Conv(in_channels , hidden_dim , kernel_size=7 , stride=4 , padding=0 , use_pool=False , use_norm=False)\n",
        "    self.conv2 = Conv(hidden_dim , hidden_dim * 2 , kernel_size=7 , stride=4 , padding=0 , use_pool=False)\n",
        "    self.conv3 = Conv(hidden_dim * 2 , hidden_dim * 4 , kernel_size=7 ,stride=4 ,padding=0 , use_pool=False)\n",
        "    self.conv4 = Conv(hidden_dim * 4 , out_channels , kernel_size=6 , stride=1 , padding=0 , use_pool=False , use_norm=False)\n",
        "\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5xPLygZmgUh"
      },
      "source": [
        "encoder_target = Encoder_Target(3 , 512).to(device)\n",
        "summary(encoder_target , (3 , 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H07kgJl7nMFi"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels , \n",
        "               z_dim = 512 , \n",
        "               hidden_dim = 32):\n",
        "    super(Generator , self).__init__()\n",
        "\n",
        "    self.encoder_target = Encoder_Target(in_channels , z_dim)\n",
        "\n",
        "    self.conv1 = Conv(in_channels , hidden_dim , use_norm=False)\n",
        "    self.conv2 = Conv(hidden_dim , hidden_dim * 2 , use_norm=False)\n",
        "    self.conv3 = Conv(hidden_dim * 2 , hidden_dim * 4)\n",
        "    self.conv4 = Conv(hidden_dim * 4 , hidden_dim * 8)\n",
        "    self.conv5 = Conv(hidden_dim * 8 , hidden_dim * 16)\n",
        "    self.conv6 = Conv(hidden_dim * 16 , hidden_dim * 32 , use_norm=False)\n",
        "    self.conv7 = Conv(hidden_dim * 32 , hidden_dim * 64 , use_norm=False)\n",
        "\n",
        "    self.last_conv = Conv(hidden_dim * 64 , z_dim , use_norm=False)\n",
        "\n",
        "\n",
        "    self.conv_mapping_1 = Conv(z_dim * 2 , hidden_dim * 64 , use_pool=False , use_norm=False)\n",
        "\n",
        "    self.convT1 = ConvT(hidden_dim * 64 , hidden_dim * 32)\n",
        "    self.convT2 = ConvT(hidden_dim * 32 , hidden_dim * 16)\n",
        "    self.convT3 = ConvT(hidden_dim * 16 , hidden_dim * 8)\n",
        "    self.convT4 = ConvT(hidden_dim * 8 , hidden_dim * 4)\n",
        "    self.convT5 = ConvT(hidden_dim * 4 , hidden_dim * 2)\n",
        "    self.convT6 = ConvT(hidden_dim * 2,  hidden_dim , use_norm=False)\n",
        "    self.convT7 = ConvT(hidden_dim , hidden_dim , use_norm=False)\n",
        "\n",
        "    \n",
        "    self.convT_last = nn.ConvTranspose2d(hidden_dim , out_channels , kernel_size=(2 , 2) , stride=(2 , 2))\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    self.last_conv1 = Conv(in_channels , out_channels , use_pool=False)\n",
        "    self.last_conv2 = Conv(out_channels , out_channels , use_pool=False , use_norm=False , use_activation=False)\n",
        "\n",
        "\n",
        "  \n",
        "  def forward(self , x , y):\n",
        "    y = self.encoder_target(y)\n",
        "\n",
        "    x1 = self.conv1(x)\n",
        "    x2 = self.conv2(x1)\n",
        "    x3 = self.conv3(x2)\n",
        "    x4 = self.conv4(x3)\n",
        "    x5 = self.conv5(x4)\n",
        "    x6 = self.conv6(x5)\n",
        "    x7 = self.conv7(x6)\n",
        "\n",
        "    x8 = self.last_conv(x7)\n",
        "\n",
        "    z = torch.cat([x8 , y] , dim=1)\n",
        "    z = self.conv_mapping_1(z)\n",
        "\n",
        "    x9 = self.convT1(z , x7)\n",
        "    x10 = self.convT2(x9 , x6)\n",
        "    x11 = self.convT3(x10 , x5)\n",
        "    x12 = self.convT4(x11 , x4)\n",
        "    x13 = self.convT5(x12 , x3)\n",
        "    x14 = self.convT6(x13 , x2)\n",
        "    x15 = self.convT7(x14 , x1)\n",
        "\n",
        "    x = self.relu(self.convT_last(x15))\n",
        "    x = self.last_conv1(x)\n",
        "    x = self.sigmoid(self.last_conv2(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk_N9dQrqi_o"
      },
      "source": [
        "x = torch.randn(2 , 3 , 512 , 512).to(device)\n",
        "y = torch.randn(2 , 3 , 512 , 512).to(device)\n",
        "generator = Generator(3 , 3).to(device)\n",
        "z = generator(x , y)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAPscxYuqwIt"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               z_dim = 512 , \n",
        "               hidden_dim = 32):\n",
        "    super(Discriminator , self).__init__()\n",
        "\n",
        "    self.encoder = Encoder_Source(in_channels , z_dim)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.linear1 = nn.Linear(z_dim * 4 , z_dim)\n",
        "    self.batchnorm1 = nn.BatchNorm1d(z_dim)\n",
        "    \n",
        "    self.linear2 = nn.Linear(z_dim , 32)\n",
        "    self.batchnorm2 = nn.BatchNorm1d(32)\n",
        "\n",
        "    self.linear3 = nn.Linear(32 , 1)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.linear1(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.linear3(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12xH2cdoWQc9"
      },
      "source": [
        "discriminator = Discriminator(in_channels).to(device)\n",
        "summary(discriminator , (3 , 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jpz_TnUWpJK"
      },
      "source": [
        "\n",
        "adv_criterion = nn.BCEWithLogitsLoss()\n",
        "recon_criterion = nn.L1Loss()\n",
        "lambda_recon = 200\n",
        "betas = (0.5 , 0.999)\n",
        "\n",
        "n_epochs = 200\n",
        "in_channels = 3\n",
        "out_channels = 3\n",
        "display_step = 5\n",
        "batch_size = 2\n",
        "lr = 0.0002\n",
        "target_shape = 512"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze3Hls0ubiUQ"
      },
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "import torchvision\n",
        "dataset = torchvision.datasets.ImageFolder(\"/content/drive/MyDrive/Maps/maps/\", transform=transform)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApOjjodObnWi"
      },
      "source": [
        "generator_x = Generator(in_channels , out_channels).to(device)\n",
        "generator_y = Generator(in_channels , out_channels).to(device)\n",
        "discriminator_x = Discriminator(in_channels , out_channels_disc).to(device)\n",
        "discriminator_y = Discriminator(in_channels , out_channels_disc).to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYeUvDmNglVl"
      },
      "source": [
        "opt_generator_x = torch.optim.Adam(generator_x.parameters() , lr=lr , betas = betas )\n",
        "opt_generator_y = torch.optim.Adam(generator_y.parameters() , lr=lr , betas = betas )\n",
        "opt_discriminator_x = torch.optim.Adam(discriminator_x.parameters() , lr=lr , betas = betas )\n",
        "opt_discriminator_y = torch.optim.Adam(discriminator_y.parameters() , lr=lr , betas = betas )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCYQqHHdfmHb"
      },
      "source": [
        "def weights_init(m):\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "  if isinstance(m, nn.BatchNorm2d):\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    torch.nn.init.constant_(m.bias, 0)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixw2b8IifqJj"
      },
      "source": [
        "generator_x = generator_x.apply(weights_init)\n",
        "generator_y = generator_y.apply(weights_init)\n",
        "discriminator_x = discriminator_x.apply(weights_init)\n",
        "discriminator_y = discriminator_y.apply(weights_init)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAUScYvHgAOi"
      },
      "source": [
        "dataloader = DataLoader(dataset , batch_size = batch_size , shuffle=True)\n",
        "input_dim = 3\n",
        "real_dim = 3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjJmMq-t3H1d"
      },
      "source": [
        "def get_gen_loss(fake_output , \n",
        "                 real , \n",
        "                 disc_fake_pred, \n",
        "                 adv_criterion = adv_criterion , \n",
        "                 recon_criterion = recon_criterion , \n",
        "                 lambda_recon = lambda_recon):\n",
        "  gen_loss = adv_criterion(fake_output , real)\n",
        "  disc_loss_ = adv_criterion(disc_fake_pred , torch.zeros_like(disc_fake_pred))\n",
        "  gen_loss = recon_criterion(fake_output , real)\n",
        "  loss = disc_loss_ + lambda_recon * gen_loss + lambda_recon * gen_loss\n",
        "  return loss"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuSzdVhVgJg_"
      },
      "source": [
        "def train():\n",
        "  mean_generator_loss = 0\n",
        "  mean_discriminator_loss = 0\n",
        "  cur_step = 0\n",
        "  for epoch in range(n_epochs):\n",
        "    for img , _ in tqdm(dataloader):\n",
        "      image_width = img.shape[3]\n",
        "      condition = img[: , : , : , :image_width//2]\n",
        "      condition = nn.functional.interpolate(condition , size = target_shape)\n",
        "      real = img[: , : , : , image_width//2:]\n",
        "      real = nn.functional.interpolate(real , size = target_shape)\n",
        "      cur_batch_size = len(condition)\n",
        "      real = real.to(device)\n",
        "      condition = condition.to(device)\n",
        "      #print(torch.max(condition) , torch.max(real))\n",
        "\n",
        "      #X = condition , Y = real\n",
        "      # generator_x => real\n",
        "      # generator_y => condition\n",
        "      # discriminator_x => real\n",
        "      # discriminator_y => condition\n",
        "\n",
        "      opt_generator_x.zero_grad()\n",
        "\n",
        "      fake_X_X_X = generator_x(condition , real)\n",
        "      fake_X_Y_X = generator_x(condition , condition)\n",
        "      \n",
        "      disc_fake_pred_X_X_X = discriminator_x(fake_X_X_X)\n",
        "      disc_fake_pred_X_Y_X = discriminator_x(fake_X_Y_X)\n",
        "\n",
        "      gen_x_loss_1 = get_gen_loss(fake_X_X_X , real , disc_fake_pred_X_X_X)\n",
        "      gen_x_loss_2 = get_gen_loss(fake_X_Y_X , real , disc_fake_pred_X_Y_X)\n",
        "\n",
        "      generator_x_loss = (gen_x_loss_1 + gen_x_loss_2)/2\n",
        "\n",
        "      generator_x_loss.backward()\n",
        "      opt_generator_x.step()\n",
        "\n",
        "      opt_generator_y.zero_grad()\n",
        "      \n",
        "      fake_Y_X_Y = generator_y(real , real)\n",
        "      fake_Y_Y_Y = generator_y(real , condition)\n",
        "\n",
        "      disc_fake_pred_Y_X_Y = discriminator_y(fake_Y_X_Y)\n",
        "      disc_fake_pred_Y_Y_Y = discriminator_y(fake_Y_Y_Y)\n",
        "\n",
        "      gen_y_loss_1 = get_gen_loss(fake_Y_X_Y , condition , disc_fake_pred_Y_X_Y)\n",
        "      gen_y_loss_2 = get_gen_loss(fake_Y_Y_Y , condition , disc_fake_pred_Y_Y_Y)\n",
        "\n",
        "      generator_y_loss = (gen_y_loss_1 + gen_y_loss_2)/2\n",
        "\n",
        "      generator_y_loss.backward()\n",
        "      opt_generator_y.step()\n",
        "\n",
        "      opt_discriminator_x.zero_grad()\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        fake_X_X_X = generator_x(condition , real)\n",
        "        fake_X_Y_X = generator_x(condition , condition)\n",
        "      disc_fake_pred_X_X_X_ = discriminator_x(fake_X_X_X)\n",
        "      disc_fake_pred_X_Y_X_ = discriminator_x(fake_X_Y_X)\n",
        "      disc_real_pred_X = discriminator_x(real)\n",
        "\n",
        "      disc_fake_pred_X_X_X_loss = adv_criterion(disc_fake_pred_X_X_X_ , torch.zeros_like(disc_fake_pred_X_X_X_))\n",
        "      disc_fake_pred_X_Y_X_loss = adv_criterion(disc_fake_pred_X_Y_X_ , torch.zeros_like(disc_fake_pred_X_Y_X_))\n",
        "      disc_real_pred_X_loss = adv_criterion(disc_real_pred_X , torch.ones_like(disc_real_pred_X))\n",
        "      \n",
        "      discriminator_x_loss = (disc_fake_pred_X_X_X_loss + disc_fake_pred_X_Y_X_loss + disc_real_pred_X_loss)/3\n",
        "\n",
        "      discriminator_x_loss.backward()\n",
        "      opt_discriminator_x.step()\n",
        "\n",
        "\n",
        "      opt_discriminator_y.zero_grad()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        fake_Y_X_Y = generator_y(real , real)\n",
        "        fake_Y_Y_Y = generator_y(real , condition)        \n",
        "\n",
        "      disc_fake_pred_Y_X_Y_ = discriminator_y(fake_Y_X_Y)\n",
        "      disc_fake_pred_Y_Y_Y_ = discriminator_y(fake_Y_Y_Y)\n",
        "      disc_real_pred_Y = discriminator_y(condition)\n",
        "\n",
        "      disc_fake_pred_Y_X_Y_loss = adv_criterion(disc_fake_pred_Y_X_Y_ , torch.zeros_like(disc_fake_pred_Y_X_Y_))\n",
        "      disc_fake_pred_Y_Y_Y_loss = adv_criterion(disc_fake_pred_Y_Y_Y_ , torch.zeros_like(disc_fake_pred_Y_Y_Y_))\n",
        "      disc_real_pred_Y_loss = adv_criterion(disc_real_pred_Y , torch.ones_like(disc_real_pred_Y))\n",
        "\n",
        "      discriminator_y_loss = (disc_fake_pred_Y_X_Y_loss + disc_fake_pred_Y_Y_Y_loss + disc_real_pred_Y_loss)/3\n",
        "      discriminator_y_loss.backward()\n",
        "      opt_discriminator_y.step()\n",
        "\n",
        "      discriminator_loss = (discriminator_x_loss + discriminator_y_loss)/2\n",
        "      generator_loss = (generator_x_loss + generator_y_loss)/2\n",
        "\n",
        "      mean_discriminator_loss += discriminator_loss.item() / display_step\n",
        "      mean_generator_loss += generator_loss.item() / display_step\n",
        "\n",
        "      if cur_step % display_step == 0:\n",
        "        if cur_step > 0:\n",
        "          print(f\"Epoch {epoch}: Step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n",
        "        else:\n",
        "          print(\"Pretrained initial state\")\n",
        "        print('condition')\n",
        "        show_tensor_images(condition, size=(input_dim, target_shape, target_shape))\n",
        "        print('real')\n",
        "        show_tensor_images(real, size=(real_dim, target_shape, target_shape) )\n",
        "        print('fake_X_X_X')\n",
        "        show_tensor_images(fake_X_X_X, size=(real_dim, target_shape, target_shape) )\n",
        "        print('fake_X_Y_X')\n",
        "        show_tensor_images(fake_X_Y_X , size=(real_dim , target_shape , target_shape))\n",
        "        print('fake_Y_X_Y')\n",
        "        show_tensor_images(fake_Y_X_Y , size=(real_dim , target_shape , target_shape))\n",
        "        print('fake_Y_Y_Y')\n",
        "        show_tensor_images(fake_Y_Y_Y , size=(real_dim , target_shape , target_shape))\n",
        "        mean_generator_loss = 0\n",
        "        mean_discriminator_loss = 0\n",
        "      cur_step += 1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1i7YLsr1Stx"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIMaFDHt-7ub"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}